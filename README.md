# ğŸ§  Multimodal RAG Pipeline for Educational Content

A fully functional **Retrieval-Augmented Generation (RAG)** pipeline built with **LangChain**, **Ollama**, and **Qdrant**, designed to query and summarize multimodal PDF documents containing **text, mathematical formulas, and diagrams**.

This project was developed as part of an **Intern Interview Assignment** to demonstrate applied **LLM and Data Science** skills.

---

## ğŸš€ Objective

To build a **console-based RAG system** capable of:

- Parsing and indexing complex educational PDFs (text + images/formulas).  
- Storing vector embeddings in **Qdrant**.  
- Using **Ollama-supported LLMs** (e.g., `mistral`, `llama3`) for context-augmented responses.  
- Demonstrating caching, summarization, and multimodal query capabilities.

---

## ğŸ§© Tech Stack

| Component | Technology Used |
|------------|------------------|
| **Programming Language** | Python |
| **Framework** | LangChain |
| **LLM Provider** | Ollama |
| **Vector Store** | Qdrant |
| **PDF Parser** | PyMuPDF (`fitz`) |
| **Embeddings Model** | `nomic-embed-text` |
| **Caching & Memory** | LangChain Prompt/Conversation Memory |

---

## ğŸ“ Repository Structure

RAG-Pipeline-Development/

â”‚

â”œâ”€â”€ data/ # Sample or indexed files

â”œâ”€â”€ index/ # Vector index storage

â”œâ”€â”€ setup_pipeline.py # Handles PDF parsing, embeddings & Qdrant setup

â”œâ”€â”€ rag_query.py # Main RAG query engine

â”œâ”€â”€ generate_local_index.py # Helper for local embedding generation

â”œâ”€â”€ requirements.txt # Python dependencies

â””â”€â”€ README.md # Documentation



---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository

git clone https://github.com/Hari99-ai/RAG-Pipeline-Development.git
cd RAG-Pipeline-Development
### 2ï¸âƒ£ Create and activate a virtual environment

python -m venv .venv
source .venv/bin/activate      # For Linux/Mac
.venv\Scripts\activate         # For Windows
### 3ï¸âƒ£ Install dependencies

pip install -r requirements.txt
### 4ï¸âƒ£ Start Qdrant (via Docker)

docker run -p 6333:6333 qdrant/qdrant
### 5ï¸âƒ£ Verify Ollama is installed
Install Ollama from https://ollama.ai/download and pull required models:

ollama pull mistral
ollama pull nomic-embed-text

## ğŸ§® Usage
ğŸ”¹ Step 1: Index the PDF

python setup_pipeline.py
Expected Output:

mathematica

âœ… Connected successfully!
ğŸ“š PDF parsed and 50 chunks indexed in Qdrant.

ğŸ”¹ Step 2: Ask Questions (RAG Query)

python rag_query.py --question "Explain the steps involved in solving a quadratic equation."
Expected Output:

Final Answer: [LLM-generated response]
Sources: [Chunk references]

ğŸ”¹ Step 3: Summarization

python rag_query.py --summarize --question "What is Arithmetic Progression?"
Output:
1. Retrieved Context Summary: [Brief summary]
2. Final RAG Answer: [LLM output]
3. 
ğŸ”¹ Step 4: Caching Demonstration
Run the same question twice to verify caching/memory usage.

ğŸ§© Features Summary

âœ… PDF parsing (text + image/formulas)

âœ… Qdrant vector storage

âœ… Ollama-based embeddings and generation

âœ… Context summarization before generation

âœ… Prompt/Conversational caching

âœ… Command-line interface (no UI required)

ğŸ“Š Example Queries

python rag_query.py --question "What does the diagram of a trapezoid represent?"
python rag_query.py --question "Who proposed the Pythagorean theorem?"
python rag_query.py --question "What is the formula associated with his discovery?"

ğŸ§‘â€ğŸ’» Author
Hari Om

ğŸ“§ hariom993126@gmail.com
